{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dog breed recognition.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Naitsirc98/Dog-breed-recognition/blob/master/Dog_breed_recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXD8sjjQLj-k",
        "colab_type": "text"
      },
      "source": [
        "# Clasificador de razas de perros\n",
        "\n",
        "Vamos a desarrollar una red que sea capaz de clasificar entre razas de perros distintas. El dataset es el [Stanford Dogs Dataset](http://vision.stanford.edu/aditya86/ImageNetDogs/main.html), reducido a 12 clases.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viRjIR8okKLE",
        "colab_type": "text"
      },
      "source": [
        "Como tengo el dataset descargado en mi drive, ejecutamos lo siguiente para acceder:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZcKFL_TMkzR",
        "colab_type": "code",
        "outputId": "a998de91-28b4-4a7e-8960-1346b66c073f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GJsa2DPRE5C",
        "colab_type": "text"
      },
      "source": [
        "Comprobamos que GPU estamos utilizando"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3JBgGOsKu44",
        "colab_type": "code",
        "outputId": "6bee8516-1540-4725-bd9a-8709ef6d40b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCaBgMS8OdiV",
        "colab_type": "text"
      },
      "source": [
        "## Cargando los datos\n",
        "\n",
        "Vamos a desarrollar una funci√≥n que nos cargue los datos necesarios y nos genere los generadores de entrenamiento y test:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KE6j7-r0OgXA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "def load_data(batch_size = 20, directory = '/content/drive/My Drive/Colab Notebooks/datasets/Dogs/Images'):\n",
        "\n",
        "    train_datagen = ImageDataGenerator(\n",
        "        rescale = 1.0 / 255,\n",
        "        shear_range = 0.2,\n",
        "        zoom_range = 0.2,\n",
        "        horizontal_flip = True\n",
        "    )\n",
        "\n",
        "    test_datagen = ImageDataGenerator(rescale = 1.0 / 255)\n",
        "\n",
        "    train_generator = train_datagen.flow_from_directory(\n",
        "        directory,\n",
        "        target_size=(256, 256),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical'\n",
        "    )\n",
        "\n",
        "    test_generator = test_datagen.flow_from_directory(\n",
        "        directory,\n",
        "        target_size=(256, 256),\n",
        "        batch_size=32,\n",
        "        class_mode='categorical'\n",
        "    )\n",
        "\n",
        "    return train_generator, test_generator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-sKhTBgofiK",
        "colab_type": "text"
      },
      "source": [
        "## Modelo\n",
        "\n",
        "Creamos el modelo con Keras:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDc5Wes9ot5Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
        "from keras.layers import Dropout, Activation, Dense\n",
        "from keras.models import Sequential\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "train_gen, test_gen = load_data()\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(filters=16, kernel_size=3, activation='relu', input_shape=(256, 256, 3)))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(filters=64, kernel_size=3, activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(filters=128, kernel_size=3, activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(filters=256, kernel_size=3, activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(GlobalAveragePooling2D())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(12, activation='softmax'))\n",
        "\n",
        "model.summary()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rR9PwKE7DiAL",
        "colab_type": "text"
      },
      "source": [
        "## O cargamos el modelo desde el disco"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKicQVcLDibQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
        "from keras.layers import Dropout, Activation, Dense\n",
        "from keras.models import Sequential\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "from keras.models import model_from_json\n",
        "\n",
        "train_gen, test_gen = load_data()\n",
        "\n",
        "# Model reconstruction from JSON file\n",
        "with open(\"/content/drive/My Drive/Colab Notebooks/modelb.json\", 'r') as f:\n",
        "    model = model_from_json(f.read())\n",
        "\n",
        "# Load weights into the new model\n",
        "model.load_weights(\"/content/drive/My Drive/Colab Notebooks/modelb.h5\")\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RefaSY5IozQU",
        "colab_type": "text"
      },
      "source": [
        "## Entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UTdMrqIo1bq",
        "colab_type": "code",
        "outputId": "8615118a-c29d-4c53-fd98-9d4d7f90f22d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 654
        }
      },
      "source": [
        "model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer=keras.optimizers.Adadelta(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "for i in range(0, 10):\n",
        "  epochs = 5\n",
        "\n",
        "  history = model.fit_generator(\n",
        "        train_gen,\n",
        "        steps_per_epoch=100,\n",
        "        epochs=epochs,\n",
        "        validation_data=test_gen,\n",
        "        validation_steps=800,\n",
        "  )\n",
        "\n",
        "  # serialize model to JSON\n",
        "  model_json = model.to_json()\n",
        "  with open(\"/content/drive/My Drive/Colab Notebooks/modelb.json\", \"w\") as json_file:\n",
        "      json_file.write(model_json)\n",
        "  # serialize weights to HDF5\n",
        "  model.save_weights(\"/content/drive/My Drive/Colab Notebooks/modelb.h5\")\n",
        "  print(\"Saved model to disk\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "100/100 [==============================] - 166s 2s/step - loss: 0.2026 - acc: 0.9325 - val_loss: 0.1394 - val_acc: 0.9547\n",
            "Epoch 2/5\n",
            "100/100 [==============================] - 160s 2s/step - loss: 0.1600 - acc: 0.9499 - val_loss: 0.1295 - val_acc: 0.9617\n",
            "Epoch 3/5\n",
            "100/100 [==============================] - 159s 2s/step - loss: 0.1769 - acc: 0.9435 - val_loss: 0.1216 - val_acc: 0.9615\n",
            "Epoch 4/5\n",
            "100/100 [==============================] - 159s 2s/step - loss: 0.1865 - acc: 0.9410 - val_loss: 0.3118 - val_acc: 0.9075\n",
            "Epoch 5/5\n",
            "100/100 [==============================] - 157s 2s/step - loss: 0.1145 - acc: 0.9604 - val_loss: 0.1462 - val_acc: 0.9447\n",
            "Saved model to disk\n",
            "Epoch 1/5\n",
            "100/100 [==============================] - 163s 2s/step - loss: 0.1198 - acc: 0.9600 - val_loss: 0.1846 - val_acc: 0.9397\n",
            "Epoch 2/5\n",
            "100/100 [==============================] - 157s 2s/step - loss: 0.1086 - acc: 0.9655 - val_loss: 0.0481 - val_acc: 0.9832\n",
            "Epoch 3/5\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.1863 - acc: 0.9417"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
